<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>FINAL_REPORT</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css" />
</head>
<body>
<h1
id="end-to-end-data-curation-workflow-for-climate-change-impact-data">End-to-End
Data Curation Workflow for Climate Change Impact Data</h1>
<h2 id="final-project-report">Final Project Report</h2>
<p><strong>Team 11 — CS598 Data Cleaning</strong><br />
<strong>University of Illinois Urbana-Champaign</strong><br />
<strong>October 2025</strong></p>
<p><strong>Team Members:</strong> - Cesar Mancillas (UIN: 651066529,
cam37@illinois.edu) - Aristofanes Cruz (UIN: 655558479,
ac163@illinois.edu) - Cesar Nava (UIN: 654326785,
can14@illinois.edu)</p>
<p><strong>Repository:</strong>
https://github.com/ceasarm7/cs598-e2e-datacuration</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>This project develops and documents an end-to-end data curation
workflow for climate change impact data, focusing on the Annual
Temperature Anomalies dataset from Our World in Data (OWID). The
workflow demonstrates systematic approaches to data acquisition, quality
assessment, cleaning, transformation, and documentation, resulting in a
reproducible, well-documented dataset suitable for climate change
research. Through the application of established data lifecycle models,
standards, and best practices, we transform raw climate data into a
quality-assured resource that supports long-term regional and global
temperature trend analysis. All artifacts, including scripts,
documentation, and cleaned datasets, are available in our GitHub
repository.</p>
<hr />
<h2 id="introduction-project-motivation-and-context">1. Introduction:
Project Motivation and Context</h2>
<h3 id="motivation">1.1 Motivation</h3>
<p>Climate change represents one of the most pressing challenges of our
time, requiring robust data-driven analysis to understand long-term
trends and regional variations. However, raw climate data often contains
quality issues that can compromise research validity, including missing
values, inconsistencies, duplicates, and outliers. This project
addresses these challenges by developing a comprehensive data curation
workflow that transforms raw climate data into a reliable,
well-documented resource for scientific analysis.</p>
<h3 id="use-case-and-research-question">1.2 Use Case and Research
Question</h3>
<p>Our primary use case (<strong>U1</strong>) guides all curation
activities:</p>
<blockquote>
<p>“Analyze long-term regional and global temperature trends to
understand the impact of climate change.”</p>
</blockquote>
<p>This use case requires: - <strong>Temporal coverage:</strong>
Historical data spanning multiple decades (1880-2023) - <strong>Spatial
coverage:</strong> Global representation across approximately 190
countries - <strong>Data quality:</strong> Clean, standardized, and
validated measurements - <strong>Reproducibility:</strong> Transparent
workflow enabling verification and extension</p>
<p>The research question we support is: <em>How have global and regional
temperature anomalies changed over time, and what patterns emerge when
comparing different countries and regions?</em></p>
<h3 id="project-scope">1.3 Project Scope</h3>
<p>This project focuses on the Annual Temperature Anomalies dataset,
which measures deviations from a baseline reference period (1951-1980)
in degrees Celsius. The curation workflow encompasses four main phases:
(1) data acquisition and modeling, (2) quality assessment, (3) cleaning
and transformation, and (4) documentation and reproducibility. All work
is conducted with transparency, reproducibility, and reuse in mind,
following established data stewardship principles.</p>
<hr />
<h2 id="dataset-profile">2. Dataset Profile</h2>
<h3 id="source-and-license">2.1 Source and License</h3>
<p>The dataset originates from <strong>Our World in Data
(OWID)</strong>, a research organization providing open-access data on
global challenges. The specific dataset, “Annual Temperature Anomalies,”
is available under a <strong>Creative Commons Attribution 4.0
International (CC BY 4.0)</strong> license, which permits sharing and
adaptation with proper attribution (Our World in Data, n.d.).</p>
<p><strong>Source URL:</strong>
https://ourworldindata.org/climate-change</p>
<h3 id="data-structure">2.2 Data Structure</h3>
<p>The dataset follows a relational model with a single primary table,
<code>TemperatureAnomalies</code>, containing four columns:</p>
<ul>
<li><strong><code>country</code></strong> (VARCHAR(100)): Full name of
the country or geographical region</li>
<li><strong><code>country_code</code></strong> (VARCHAR(3)): ISO 3166-1
alpha-3 (ISO-3) three-letter country code</li>
<li><strong><code>year</code></strong> (INT): Calendar year for the
temperature record (range: 1880-2023)</li>
<li><strong><code>temperature_anomaly</code></strong> (DECIMAL(7,6)):
Annual deviation from baseline in degrees Celsius</li>
</ul>
<p>The composite primary key <code>(country_code, year)</code> ensures
uniqueness of country-year pairs. The complete schema definition is
available in <code>artifacts/schema_definition.sql</code>.</p>
<h3 id="data-extent">2.3 Data Extent</h3>
<ul>
<li><strong>Spatial Coverage:</strong> Global, encompassing
approximately 190 countries and regions</li>
<li><strong>Temporal Coverage:</strong> 1880-2023 (144 years)</li>
<li><strong>Total Records:</strong> Approximately 53,000 records (after
cleaning)</li>
<li><strong>Geographic Scope:</strong> Worldwide, with some regional
aggregates (e.g., “World”)</li>
</ul>
<h3 id="data-characteristics">2.4 Data Characteristics</h3>
<p>Temperature anomalies represent deviations from the 1951-1980
baseline period, measured in degrees Celsius. Positive values indicate
warmer-than-baseline conditions, while negative values indicate cooler
conditions. The dataset includes both country-specific and aggregate
regional measurements, enabling both national and global trend
analysis.</p>
<hr />
<h2 id="data-curation-workflow">3. Data Curation Workflow</h2>
<p>Our data curation workflow follows a systematic four-phase approach,
with each phase producing documented artifacts that enable
reproducibility and verification. The complete workflow is documented in
<code>artifacts/provenance_workflow.md</code>.</p>
<h3 id="phase-1-data-acquisition-and-modeling">3.1 Phase 1: Data
Acquisition and Modeling</h3>
<p><strong>Objective:</strong> Acquire raw data and establish target
schema for integration.</p>
<p><strong>Activities:</strong> 1. Downloaded the dataset from Our World
in Data website 2. Verified data integrity and source authenticity 3.
Defined relational schema (<code>artifacts/schema_definition.sql</code>)
with: - Table structure: <code>TemperatureAnomalies</code> - Primary
key: <code>(country_code, year)</code> - Data types and constraints</p>
<p><strong>Artifacts:</strong>
<code>artifacts/schema_definition.sql</code></p>
<p>This phase establishes the foundation for all subsequent curation
activities by defining the target data model and ensuring structural
consistency.</p>
<h3 id="phase-2-quality-assessment">3.2 Phase 2: Quality Assessment</h3>
<p><strong>Objective:</strong> Identify and document data quality
issues.</p>
<p><strong>Activities:</strong> 1. Conducted initial data profiling 2.
Identified quality issues: - <strong>Missing data:</strong> 4,512
records (8.5%) in <code>temperature_anomaly</code>, clustered in early
decades (pre-1930) - <strong>Missing country codes:</strong> 15 records
requiring manual resolution - <strong>Duplicate records:</strong> 87
duplicate (country, year) pairs - <strong>Outliers:</strong> Extreme
temperature values (max: 9.998°C, min: -3.5°C) requiring validation 3.
Documented findings in quality profile report</p>
<p><strong>Artifacts:</strong>
<code>quality_profile_report.txt</code></p>
<p>The quality assessment revealed systematic issues requiring targeted
cleaning strategies, particularly around missing data in historical
periods and country code standardization.</p>
<h3 id="phase-3-cleaning-and-transformation">3.3 Phase 3: Cleaning and
Transformation</h3>
<p>We implemented a two-stage cleaning pipeline combining interactive
(OpenRefine) and automated (Python) approaches.</p>
<h4 id="openrefine-pipeline">3.3.1 OpenRefine Pipeline</h4>
<p><strong>Tool:</strong> OpenRefine v3.9.5</p>
<p><strong>Operations</strong> (documented in
<code>artifacts/openrefine_operations.json</code>): 1. <strong>Country
code transformation:</strong> Standardized to uppercase, removed
non-alphabetic characters 2. <strong>Type coercion:</strong> Converted
year and temperature anomaly to numeric types 3. <strong>Composite key
creation:</strong> Generated <code>key_code_year</code> for
deduplication 4. <strong>Duplicate removal:</strong> Eliminated obvious
duplicates based on country name 5. <strong>Outlier flagging:</strong>
Created <code>outlier_flag</code> column for values |anomaly| &gt;
5°C</p>
<p><strong>Output:</strong>
<code>artifacts/temperature_anomalies_refine_clean_base.csv</code></p>
<h4 id="pythonpandas-pipeline">3.3.2 Python/Pandas Pipeline</h4>
<p><strong>Tool:</strong> Python 3.8+ with Pandas 2.0+</p>
<p><strong>Script:</strong>
<code>artifacts/initial_cleaning_script.py</code></p>
<p><strong>Processing Steps:</strong> 1. <strong>Column
standardization:</strong> Mapped source column names to standard schema
2. <strong>Country code normalization:</strong> - Converted to uppercase
- Mapped aliases to ISO-3 (e.g., “UK” → “GBR”, “US” → “USA”) - Validated
ISO-3 format (exactly 3 uppercase letters) - Set invalid codes to NULL
3. <strong>Type coercion:</strong> Ensured proper data types (string,
integer, decimal) 4. <strong>Deduplication:</strong> Removed 87
duplicate (country_code, year) pairs, keeping first occurrence 5.
<strong>Quality reporting:</strong> Generated data quality metrics
without data mutation</p>
<p><strong>Outputs:</strong> -
<code>artifacts/temperature_anomalies_initial_clean.csv</code> (cleaned
dataset) -
<code>artifacts/temperature_anomalies_initial_dq_report.csv</code>
(quality metrics)</p>
<p>The two-stage approach leverages OpenRefine’s interactive
capabilities for exploratory cleaning and Python’s automation for
systematic standardization, ensuring both flexibility and
reproducibility.</p>
<h3 id="phase-4-documentation-and-reproducibility">3.4 Phase 4:
Documentation and Reproducibility</h3>
<p><strong>Objective:</strong> Create comprehensive documentation
enabling reuse and verification.</p>
<p><strong>Activities:</strong> 1. Created detailed data dictionary
(<code>artifacts/data_dictionary.md</code>) 2. Generated DataCite
metadata (<code>artifacts/metadata_datacite.json</code>) 3. Documented
complete provenance chain
(<code>artifacts/provenance_workflow.md</code>) 4. Created environment
specification (<code>requirements.txt</code>) 5. Developed lifecycle
analysis (<code>artifacts/lifecycle_analysis.md</code>) 6. Documented
identifier systems (<code>artifacts/identifier_systems.md</code>) 7.
Analyzed data concepts and practices
(<code>artifacts/data_concepts_practices.md</code>)</p>
<p><strong>Artifacts:</strong> Complete documentation suite covering all
aspects of the curation process.</p>
<p>This phase ensures the dataset is not only cleaned but also fully
documented, enabling future researchers to understand, verify, and
extend our work.</p>
<hr />
<h2 id="lifecycle-model-analysis">4. Lifecycle Model Analysis</h2>
<p>Our workflow aligns with established data lifecycle models,
demonstrating systematic progression through curation phases. The
complete analysis is available in
<code>artifacts/lifecycle_analysis.md</code>.</p>
<h3 id="dcc-curation-lifecycle-model">4.1 DCC Curation Lifecycle
Model</h3>
<p>The Digital Curation Centre (DCC) Curation Lifecycle Model provides a
framework for understanding data curation stages (Digital Curation
Centre, n.d.). Our workflow maps to key DCC phases:</p>
<ul>
<li><strong>Create/Receive:</strong> Data acquisition from OWID</li>
<li><strong>Appraise/Select:</strong> Quality assessment and issue
identification</li>
<li><strong>Ingest:</strong> Schema definition and modeling</li>
<li><strong>Preserve:</strong> Cleaning and transformation (OpenRefine +
Python)</li>
<li><strong>Store:</strong> Output generation and version control</li>
<li><strong>Access/Reuse:</strong> Documentation and GitHub
dissemination</li>
<li><strong>Transform/Describe:</strong> Ongoing throughout
workflow</li>
</ul>
<p>This alignment ensures our curation follows recognized best practices
for long-term data preservation and reuse.</p>
<h3 id="oais-reference-model">4.2 OAIS Reference Model</h3>
<p>The Open Archival Information System (OAIS) model describes
functional entities for digital preservation (Consultative Committee for
Space Data Systems, 2012). Our workflow implements OAIS concepts:</p>
<ul>
<li><strong>Ingest Function:</strong> Receiving raw OWID data (SIP) and
generating cleaned dataset (AIP)</li>
<li><strong>Archival Storage:</strong> Organized <code>/artifacts</code>
directory with version control</li>
<li><strong>Data Management:</strong> Schema definition, metadata, and
quality reports</li>
<li><strong>Access Function:</strong> GitHub repository providing public
access (DIP)</li>
<li><strong>Preservation Planning:</strong> Standard formats (CSV, SQL)
and metadata (DataCite)</li>
<li><strong>Administration:</strong> License compliance (CC BY 4.0) and
quality assurance</li>
</ul>
<p>The OAIS model emphasizes preservation and access, which our workflow
addresses through standard formats, comprehensive metadata, and public
dissemination.</p>
<h3 id="lifecycle-continuity">4.3 Lifecycle Continuity</h3>
<p>Our workflow demonstrates iterative refinement across lifecycle
phases: initial assessment identified issues, cleaning addressed them,
quality reporting validated improvements, and documentation enabled
reuse. This iterative approach ensures data quality improves at each
stage while maintaining transparency throughout the process.</p>
<hr />
<h2 id="course-concepts-integration">5. Course Concepts Integration</h2>
<p>Our project addresses all required course concepts, demonstrating
comprehensive understanding of data curation principles and
practices.</p>
<h3 id="data-lifecycle-m1">5.1 Data Lifecycle (M1)</h3>
<p>As detailed in Section 4, our workflow maps to both DCC and OAIS
lifecycle models, demonstrating systematic progression from acquisition
through preservation and dissemination. The lifecycle analysis document
(<code>artifacts/lifecycle_analysis.md</code>) provides detailed
phase-by-phase mapping and justification.</p>
<h3 id="ethical-legal-and-policy-constraints-m2">5.2 Ethical, Legal, and
Policy Constraints (M2)</h3>
<p><strong>License Compliance:</strong> The source dataset (OWID) uses
CC BY 4.0 license, which we maintain in all derived works. All outputs
include proper attribution to Our World in Data, ensuring legal
compliance and ethical data use.</p>
<p><strong>Data Privacy:</strong> The dataset contains no personal or
sensitive information—only publicly available climate measurements. No
privacy concerns arise.</p>
<p><strong>Ethical Considerations:</strong> The dataset supports climate
change research, a critical public good. Our curation practices ensure
data accuracy and transparency, supporting ethical scientific
research.</p>
<p><strong>Documentation:</strong> Ethical and legal considerations are
documented in <code>artifacts/provenance_workflow.md</code> (Ethical and
Legal Considerations section).</p>
<h3 id="data-models-and-abstractions-m3-5">5.3 Data Models and
Abstractions (M3-5)</h3>
<p><strong>Relational Data Model:</strong> We employ a relational model
with a single table (<code>TemperatureAnomalies</code>) and composite
primary key <code>(country_code, year)</code>. The schema is defined in
<code>artifacts/schema_definition.sql</code>.</p>
<p><strong>Abstraction Levels:</strong> - <strong>Physical:</strong> CSV
files stored in repository - <strong>Logical:</strong> Relational schema
with defined structure, types, and constraints -
<strong>Conceptual:</strong> Entities (Countries, Years, Temperature
Anomalies) and relationships</p>
<p><strong>Justification:</strong> The relational model provides
structure, enables integration with other datasets, and supports
efficient querying. The composite primary key naturally reflects the
data’s structure (country-year pairs).</p>
<p><strong>Documentation:</strong> Complete data model description in
<code>artifacts/data_dictionary.md</code> and
<code>artifacts/schema_definition.sql</code>.</p>
<h3 id="data-integration-and-cleaning-m6">5.4 Data Integration and
Cleaning (M6)</h3>
<p><strong>Integration Issues Identified:</strong> 1. <strong>Column
name heterogeneity:</strong> Source used “Entity” and “Code” vs. our
standard “country” and “country_code” 2. <strong>Country code
inconsistencies:</strong> Variations like “UK” vs. “GBR”, “US” vs. “USA”
3. <strong>Type inconsistencies:</strong> String vs. numeric types for
year and temperature 4. <strong>Missing data:</strong> 8.5% missing
temperature values, clustered in early decades</p>
<p><strong>Cleaning Processes:</strong> - <strong>OpenRefine:</strong>
Interactive normalization and type coercion -
<strong>Python/Pandas:</strong> Automated standardization, alias
mapping, deduplication - <strong>Quality reporting:</strong> Metrics
generation without data mutation</p>
<p><strong>Results:</strong> Removed 87 duplicates, standardized all
country codes to ISO-3, enforced type consistency, and documented all
missing values.</p>
<p><strong>Documentation:</strong> Complete cleaning processes
documented in <code>artifacts/provenance_workflow.md</code> and
<code>artifacts/initial_cleaning_script.py</code>.</p>
<h3 id="data-concepts-m7---basic-representation-model">5.5 Data Concepts
(M7) - Basic Representation Model</h3>
<p>The Basic Representation Model (BRM) distinguishes between
Information Object (conceptual), Data Object (physical), and
Representation Information (metadata) (Consultative Committee for Space
Data Systems, 2012).</p>
<p><strong>Our Application:</strong> - <strong>Information
Object:</strong> Annual temperature anomalies for countries/regions -
<strong>Data Object:</strong> CSV files with tabular structure -
<strong>Representation Information:</strong> Schema, data dictionary,
metadata, provenance</p>
<p><strong>BRM Layers:</strong> 1. Physical: CSV files, file system, Git
repository 2. Logical: Relational schema, data types, constraints 3.
Conceptual: Entities and relationships 4. Semantic: Meaning (temperature
deviation from baseline)</p>
<p><strong>Value-Level vs. Structure-Level:</strong> -
<strong>Value-level:</strong> Individual data points, missing values,
outliers (addressed through cleaning) -
<strong>Structure-level:</strong> Schema design, relationships,
constraints (addressed through modeling)</p>
<p><strong>Documentation:</strong> Complete BRM analysis in
<code>artifacts/data_concepts_practices.md</code>.</p>
<h3 id="metadata-and-data-documentation-m8">5.6 Metadata and Data
Documentation (M8)</h3>
<p><strong>DataCite Metadata:</strong> We created comprehensive DataCite
v4 metadata (<code>artifacts/metadata_datacite.json</code>) including: -
Creators, titles, descriptions - Subjects, formats, sizes - Rights (CC
BY 4.0) - Related identifiers (source URL, repository) - Geographic
coverage</p>
<p><strong>Data Dictionary:</strong> Detailed codebook
(<code>artifacts/data_dictionary.md</code>) providing: - Column
descriptions with types and constraints - Example values and valid
ranges - Missing value documentation - Data quality metrics - Usage
notes and limitations</p>
<p><strong>Justification:</strong> DataCite is a recognized standard for
research data metadata, ensuring interoperability and discoverability.
The data dictionary provides essential interpretation information
following BRM principles.</p>
<h3 id="identity-and-identifier-systems-m9">5.7 Identity and Identifier
Systems (M9)</h3>
<p><strong>Primary Identifier:</strong> ISO 3166-1 alpha-3 (ISO-3)
country codes</p>
<p><strong>Justification:</strong> 1. <strong>International
standard:</strong> Widely recognized and maintained by ISO 2.
<strong>Stability:</strong> Codes persist even when country names change
3. <strong>Uniqueness:</strong> One code per country, no ambiguity 4.
<strong>Interoperability:</strong> Compatible with other climate
datasets (World Bank, UN) 5. <strong>Compactness:</strong> Three
characters, efficient yet readable</p>
<p><strong>Composite Primary Key:</strong>
<code>(country_code, year)</code> uniquely identifies each record</p>
<p><strong>Justification:</strong> - Natural key reflecting data
structure - Enables efficient queries and joins - Prevents duplicate
country-year pairs - Supports temporal analysis</p>
<p><strong>Implementation:</strong> Standardized all country codes to
ISO-3 format, mapped aliases (e.g., “UK” → “GBR”), and validated format.
Removed 87 duplicates based on composite key.</p>
<p><strong>Documentation:</strong> Complete identifier system analysis
in <code>artifacts/identifier_systems.md</code>, including alternatives
considered and justification.</p>
<h3 id="standards-and-standardization-m11">5.8 Standards and
Standardization (M11)</h3>
<p><strong>Standards Used:</strong> 1. <strong>ISO 3166-1
alpha-3:</strong> Country code standard (justified in M9) 2.
<strong>DataCite Schema v4:</strong> Metadata standard (justified in M8)
3. <strong>SQL:</strong> Schema definition language (standard relational
model) 4. <strong>CSV:</strong> Data exchange format (universal
compatibility)</p>
<p><strong>Justification:</strong> - <strong>ISO-3:</strong>
International recognition, interoperability with climate datasets -
<strong>DataCite:</strong> Research data metadata standard, ensures
discoverability - <strong>SQL:</strong> Standard schema language,
enables database integration - <strong>CSV:</strong> Universal format,
tool-agnostic, human-readable</p>
<p><strong>Standardization Benefits:</strong> - Enables integration with
other datasets - Facilitates automated processing - Supports long-term
preservation - Ensures interoperability</p>
<h3 id="workflow-automation-provenance-and-reproducibility-m12">5.9
Workflow Automation, Provenance, and Reproducibility (M12)</h3>
<p><strong>Automation:</strong> Python script
(<code>artifacts/initial_cleaning_script.py</code>) automates: - Column
standardization - Country code normalization - Type coercion -
Deduplication - Quality reporting</p>
<p><strong>Provenance:</strong> Complete lineage documented in
<code>artifacts/provenance_workflow.md</code>: - Source data →
OpenRefine → Python → Cleaned outputs - All transformations documented -
Version information maintained</p>
<p><strong>Reproducibility:</strong> - Environment specification
(<code>requirements.txt</code>) - Step-by-step instructions - All
operations exported (OpenRefine JSON) - Version-controlled
repository</p>
<p><strong>Transparency:</strong> Every transformation is documented,
enabling verification and extension of our work.</p>
<h3 id="data-practices-m13">5.10 Data Practices (M13)</h3>
<p>Our project demonstrates alignment with established data practices
research (Borgman, 2015):</p>
<p><strong>Data Curation Practices:</strong> -
<strong>Documentation:</strong> Comprehensive metadata and data
dictionary - <strong>Quality Assurance:</strong> Systematic assessment
and cleaning - <strong>Standardization:</strong> ISO-3 codes, DataCite
metadata</p>
<p><strong>Data Stewardship Practices:</strong> -
<strong>Transparency:</strong> Complete provenance chain, open
repository - <strong>Preservation:</strong> Standard formats, persistent
metadata - <strong>Ethical Practice:</strong> License compliance, proper
attribution</p>
<p><strong>Research Data Management Practices:</strong> -
<strong>Organization:</strong> Structured directory, consistent naming -
<strong>Reproducibility:</strong> Automated scripts, environment specs -
<strong>Collaboration:</strong> Team workflow, version control</p>
<p><strong>FAIR Principles Alignment:</strong> -
<strong>Findable:</strong> GitHub repository, DataCite metadata -
<strong>Accessible:</strong> Public access, standard formats -
<strong>Interoperable:</strong> Standard schemas, ISO codes -
<strong>Reusable:</strong> Complete documentation, clear license</p>
<p><strong>Documentation:</strong> Complete data practices analysis in
<code>artifacts/data_concepts_practices.md</code>.</p>
<h3 id="dissemination-and-communication-m15">5.11 Dissemination and
Communication (M15)</h3>
<p><strong>GitHub Repository:</strong>
https://github.com/ceasarm7/cs598-e2e-datacuration</p>
<p><strong>Repository Structure:</strong> - <code>/artifacts</code>: All
scripts, data, and documentation - <code>README.md</code>: Project
overview and instructions - <code>requirements.txt</code>: Environment
specification - Organized, self-contained structure</p>
<p><strong>Self-Contained Package:</strong> - All artifacts included -
Complete documentation - Reproducibility instructions - No external
dependencies beyond standard tools</p>
<p><strong>Accessibility:</strong> - Public repository - Clear
documentation - Usage guidelines - Contact information</p>
<p>This structure ensures the project is understandable, reproducible,
and reusable by future researchers.</p>
<hr />
<h2 id="findings-problems-encountered-and-lessons-learned">6. Findings,
Problems Encountered, and Lessons Learned</h2>
<h3 id="key-findings">6.1 Key Findings</h3>
<p><strong>Data Quality Issues:</strong> 1. <strong>Missing
Data:</strong> 4,512 records (8.5%) missing temperature values,
primarily in early decades (pre-1930) and under-reported regions. This
clustering suggests historical data collection limitations rather than
random missingness.</p>
<ol start="2" type="1">
<li><p><strong>Duplicates:</strong> 87 duplicate (country, year) pairs
identified, likely from data integration processes or source updates.
These duplicates could lead to incorrect aggregations if not
addressed.</p></li>
<li><p><strong>Outliers:</strong> Extreme temperature values (max:
9.998°C, min: -3.5°C) flagged for review. While some may be valid
(regional extremes), others may indicate measurement errors requiring
domain expert validation.</p></li>
<li><p><strong>Country Code Inconsistencies:</strong> 15 missing codes
and numerous format variations (e.g., “UK” vs. “GBR”) requiring
standardization for integration with other datasets.</p></li>
</ol>
<p><strong>Data Characteristics:</strong> - Strong temporal coverage
(1880-2023) enabling long-term trend analysis - Comprehensive spatial
coverage (~190 countries) supporting regional comparisons - Consistent
baseline period (1951-1980) ensuring comparability - Standard units
(degrees Celsius) facilitating interpretation</p>
<h3 id="problems-encountered">6.2 Problems Encountered</h3>
<p><strong>Technical Challenges:</strong></p>
<ol type="1">
<li><p><strong>Column Name Heterogeneity:</strong> Source data used
different column names (“Entity”, “Code”) than our target schema
(“country”, “country_code”). Required mapping logic to handle both
formats.</p></li>
<li><p><strong>Country Code Variations:</strong> Multiple
representations of the same country (e.g., “UK”, “GB”, “GBR” for United
Kingdom) required comprehensive alias mapping to ISO-3
standard.</p></li>
<li><p><strong>Type Inconsistencies:</strong> Some numeric fields stored
as strings, requiring careful type coercion with error
handling.</p></li>
<li><p><strong>Missing Data Patterns:</strong> Clustered missing data in
early decades complicated imputation decisions. We chose to preserve
NULLs rather than impute, maintaining transparency.</p></li>
</ol>
<p><strong>Workflow Challenges:</strong></p>
<ol type="1">
<li><p><strong>Tool Integration:</strong> Combining OpenRefine
(interactive) and Python (automated) required careful handoff between
stages, ensuring data format compatibility.</p></li>
<li><p><strong>Reproducibility:</strong> Initial workflow relied on
manual OpenRefine operations. Exported operations JSON enables
reproducibility, but full automation would require additional
scripting.</p></li>
<li><p><strong>Documentation Scope:</strong> Balancing comprehensive
documentation with practical usability required iterative refinement of
documentation structure.</p></li>
</ol>
<h3 id="lessons-learned">6.3 Lessons Learned</h3>
<p><strong>Data Curation Principles:</strong></p>
<ol type="1">
<li><p><strong>Standardization is Critical:</strong> Using ISO-3 country
codes from the start would have prevented many cleaning challenges.
Standards provide stability and interoperability.</p></li>
<li><p><strong>Documentation Enables Reuse:</strong> Comprehensive
documentation (metadata, data dictionary, provenance) is essential for
data reuse. Future researchers need complete context to use data
effectively.</p></li>
<li><p><strong>Quality Assessment First:</strong> Systematic quality
assessment before cleaning informed our strategy and prevented
unnecessary transformations.</p></li>
<li><p><strong>Transparency Over Perfection:</strong> Preserving NULLs
and documenting issues is preferable to aggressive imputation that might
introduce bias.</p></li>
<li><p><strong>Automation Supports Reproducibility:</strong> Automated
Python scripts ensure consistent results and enable verification, while
interactive tools (OpenRefine) support exploratory cleaning.</p></li>
</ol>
<p><strong>Technical Lessons:</strong></p>
<ol type="1">
<li><p><strong>Two-Stage Cleaning:</strong> Combining interactive
(OpenRefine) and automated (Python) approaches leverages strengths of
both: exploration and reproducibility.</p></li>
<li><p><strong>Composite Keys:</strong> Natural composite keys
<code>(country_code, year)</code> reflect data structure and enable
efficient operations.</p></li>
<li><p><strong>Version Control:</strong> Git repository tracks changes
and enables collaboration, essential for team projects.</p></li>
<li><p><strong>Environment Specification:</strong>
<code>requirements.txt</code> ensures reproducible Python environment
across systems.</p></li>
</ol>
<h3 id="next-steps-and-future-work">6.4 Next Steps and Future Work</h3>
<p><strong>Immediate Enhancements:</strong> 1. <strong>Statistical
Outlier Handling:</strong> Implement z-score or IQR-based outlier
detection per country, with conservative winsorization or NULL
replacement for extreme values (|z| &gt; 3).</p>
<ol start="2" type="1">
<li><p><strong>Missing Value Imputation:</strong> Apply per-country
linear interpolation for missing values, with forward/backward fill for
edge cases only when justified by domain knowledge.</p></li>
<li><p><strong>Comprehensive DQ Report:</strong> Expand quality
reporting to include outlier statistics, imputation summaries, and
validation metrics.</p></li>
</ol>
<p><strong>Long-Term Improvements:</strong> 1. <strong>Workflow
Automation:</strong> Fully automate OpenRefine operations through
scripting or alternative tools, eliminating manual steps.</p>
<ol start="2" type="1">
<li><p><strong>Enhanced Metadata:</strong> Add temporal and spatial
coverage details to metadata, improving discoverability.</p></li>
<li><p><strong>Data Integration:</strong> Integrate with complementary
datasets (e.g., GDP, population) using ISO-3 codes as join
keys.</p></li>
<li><p><strong>Validation Framework:</strong> Develop automated
validation tests ensuring data quality in future updates.</p></li>
<li><p><strong>Versioning System:</strong> Implement semantic versioning
for dataset releases, tracking changes over time.</p></li>
</ol>
<p><strong>Research Applications:</strong> - Long-term temperature trend
analysis - Regional climate change comparisons - Integration with
socioeconomic datasets - Machine learning applications for climate
prediction</p>
<hr />
<h2 id="conclusion">7. Conclusion</h2>
<p>This project demonstrates a comprehensive end-to-end data curation
workflow for climate change impact data, transforming raw data from Our
World in Data into a cleaned, standardized, and well-documented dataset
suitable for scientific research. Through systematic application of data
lifecycle models, established standards, and best practices, we created
a reproducible workflow that addresses data quality issues while
maintaining transparency and enabling future reuse.</p>
<p>Our workflow successfully addresses all required course concepts
(M1-M15), from data lifecycle mapping to dissemination strategies. The
resulting dataset, documentation, and artifacts provide a foundation for
climate change research while demonstrating principles of data
stewardship, reproducibility, and open science.</p>
<p>The project reveals both the challenges and opportunities in climate
data curation: missing data in historical periods, inconsistencies
requiring standardization, and the need for comprehensive documentation.
However, systematic approaches to quality assessment, cleaning, and
documentation can transform raw data into reliable research
resources.</p>
<p>All artifacts—scripts, documentation, cleaned datasets, and
metadata—are available in our GitHub repository, enabling verification,
extension, and reuse. This project contributes to the broader goal of
making climate data more accessible, reliable, and useful for
understanding and addressing climate change.</p>
<hr />
<h2 id="references">References</h2>
<p>Ball, A. (2010). <em>Review of Data Management Lifecycle Models</em>.
University of Bath.
https://www.dcc.ac.uk/sites/default/files/documents/publications/reports/ball_review_lifecycle_models.pdf</p>
<p>Borgman, C. L. (2015). <em>Big Data, Little Data, No Data:
Scholarship in the Networked World</em>. MIT Press.</p>
<p>Borgman, C. L., Scharnhorst, A., &amp; Golshan, M. S. (2019). Digital
data archives as knowledge infrastructures: Mediating data sharing and
reuse. <em>Journal of the Association for Information Science and
Technology</em>, 70(8), 888-904. https://doi.org/10.1002/asi.24172</p>
<p>Consultative Committee for Space Data Systems. (2012). <em>Reference
Model for an Open Archival Information System (OAIS)</em> (ISO
14721:2012). https://public.ccsds.org/pubs/650x0m2.pdf</p>
<p>Digital Curation Centre. (n.d.). <em>DCC Curation Lifecycle
Model</em>. https://www.dcc.ac.uk/guidance/curation-lifecycle-model</p>
<p>International Organization for Standardization. (2020). <em>ISO
3166-1:2020 - Codes for the representation of names of countries and
their subdivisions - Part 1: Country codes</em>.
https://www.iso.org/iso-3166-country-codes.html</p>
<p>Leonelli, S. (2016). <em>Data-Centric Biology: A Philosophical
Study</em>. University of Chicago Press.</p>
<p>Our World in Data. (n.d.). <em>Climate Change Data</em>.
https://ourworldindata.org/climate-change</p>
<p>Pasquetto, I. V., Randles, B. M., &amp; Borgman, C. L. (2017). On the
reuse of scientific data. <em>Data Science Journal</em>, 16, 8.
https://doi.org/10.5334/dsj-2017-008</p>
<p>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G.,
Axton, M., Baak, A., Blomberg, N., Boiten, J. W., da Silva Santos, L.
B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M.,
Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B.
(2016). The FAIR Guiding Principles for scientific data management and
stewardship. <em>Scientific Data</em>, 3, 160018.
https://doi.org/10.1038/sdata.2016.18</p>
<hr />
<h2 id="appendix-artifacts-reference">Appendix: Artifacts Reference</h2>
<p>All artifacts referenced in this report are available in the GitHub
repository: https://github.com/ceasarm7/cs598-e2e-datacuration</p>
<p><strong>Key Artifacts:</strong> -
<code>artifacts/schema_definition.sql</code> - Database schema -
<code>artifacts/initial_cleaning_script.py</code> - Python cleaning
pipeline - <code>artifacts/openrefine_operations.json</code> -
OpenRefine operations - <code>artifacts/data_dictionary.md</code> - Data
dictionary/codebook - <code>artifacts/metadata_datacite.json</code> -
DataCite metadata - <code>artifacts/provenance_workflow.md</code> -
Complete workflow documentation -
<code>artifacts/lifecycle_analysis.md</code> - Lifecycle model mapping -
<code>artifacts/identifier_systems.md</code> - Identifier system
analysis - <code>artifacts/data_concepts_practices.md</code> - BRM and
data practices analysis - <code>quality_profile_report.txt</code> -
Quality assessment report - <code>requirements.txt</code> - Python
dependencies - <code>README.md</code> - Project overview</p>
<p><strong>Cleaned Datasets:</strong> -
<code>artifacts/temperature_anomalies_initial_clean.csv</code> - Final
cleaned dataset -
<code>artifacts/temperature_anomalies_initial_dq_report.csv</code> -
Quality metrics</p>
<hr />
<p><strong>Word Count:</strong> Approximately 2,400 words</p>
</body>
</html>
